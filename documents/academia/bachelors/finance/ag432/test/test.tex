\documentclass[11pt, english]{article}              
        \usepackage{geometry}
                \geometry{                          
                        a4paper,total={210mm,297mm},
                        tmargin=40.8mm,
                        bmargin=40.8mm,
                        lmargin=32.6mm,
                        rmargin=32.6mm,
                }

        \usepackage{titlesec}         
                \titleformat{\section}
                        {\normalfont\fontsize{18}{16}\bfseries}{\thesection}{0.5em}{}
                \titleformat{\subsection}
                        {\normalfont\fontsize{14}{16}\bfseries}{\thesubsection}{1em}{}
                \titleformat{\subsubsection}
                        {\normalfont\fontsize{11}{16}\bfseries}{\thesubsubsection}{1em}{}

        \usepackage{longtable}
        \usepackage{multirow}

        \usepackage[labelfont=bf,textfont=bf,font=small,skip=8pt]{caption}

        \setlength{\parindent}{0pt}
        \renewcommand{\baselinestretch}{1.25}
       \usepackage{setspace}

        \usepackage{amsmath}
        \usepackage{amssymb}

        \usepackage{graphicx}

\begin{document}

\pagenumbering{gobble}

        \title{\textsc{AG432 Financial Quantitative Methods\\ Coursework Test}}
        \author{\textsc{Lewis Britton}}
        \date{\textsc{Academic Year 2020/2021}}
        \maketitle

\newpage

\pagenumbering{roman}

        \renewcommand{\contentsname}{Table of Contents}

        \tableofcontents

\newpage

\pagenumbering{arabic} 

\section{Variance-Covariance}

	\textit{Given a row vectors of portfolio weights for three companies} $v=[0.1\ 0.3\ 0.6]$ \textit{and the variance-covariance matrix for the three companies} $S=\left[\begin{matrix}1.5\\-0.1\\1\end{matrix}\ \begin{matrix}-0.1\\1\\2\end{matrix}\ \begin{matrix}1\\2\\1.5\end{matrix}\right]$, \textit{calculate the variance of the portfolio.}

	$$\sigma_p^2=W_pcov_pW_p'$$
	$$\therefore\sigma_p^2=W_pcov_pW_p'=[w_1\ w_2\ w_3]\left[\begin{matrix}\sigma_1^2\\\sigma_{1,2}\\\sigma_{1,3}\end{matrix}\ \begin{matrix}\sigma_{1,2}\\\sigma_2^2\\\sigma_{2,3}\end{matrix}\ \begin{matrix}\sigma_{1,3}\\\sigma_{2,3}\\\sigma_3^2\end{matrix}\right]\begin{bmatrix}w_1\\w_2\\w_3\end{bmatrix}$$
	$$=[0.1\ 0.3\ 0.6]\left[\begin{matrix}1.5\\-0.1\\1\end{matrix}\ \begin{matrix}-0.1\\1\\2\end{matrix}\ \begin{matrix}1\\2\\1.5\end{matrix}\right]\begin{bmatrix}0.1\\0.3\\0.6\end{bmatrix}$$
	$$=\left[(0.15-0.03+0.6)\ (-0.1+0.3+1.2)\ (0.1+0.6+0.9)\right]\begin{bmatrix}0.1\\0.3\\0.6\end{bmatrix}$$
	$$=(0.15-0.03+0.6)0.1\ (-0.1+0.3+1.2)0.3\ (0.1+0.6+0.9)0.6$$
	$$=0.072+0.447+0.96$$
	$$\sigma_p^2=1.479$$

\newpage 

\section{Errors}

	\textit{Use the concept of Type I and Type II errors to explain bank lending behaviour before and after the financial crisis in 2008}

	\subsection{Introduction to Errors}

		\begin{table}[h]
                        \scriptsize 
                        \renewcommand{\arraystretch}{1.25}
                \begin{center}
                \begin{tabular}{p{4cm}p{4cm}p{4cm}}
                        \hline
                        \hline
                        \multicolumn{3}{c}{\textbf{Hypotheses}}\\
                        \hline
                        \hline
                        \multicolumn{3}{ p{12cm} }{\textbf{H$_0$: }Null Hypothesis \textit{Aim to Reject; may Fail-to-Reject; rarely Accept}\newline \textbf{H$\mathrm{_A}$: }Alternative Hypothesis \textit{Favour in the case of Rejection of the Null}}\\
                        \hline
                        \multicolumn{3}{ p{12cm} }{\textbf{Type I Error: }Rejection of null hypothesis when it is true\newline \textbf{Type II Error: }Failure-to-reject null hypothesis when it is false}\\
                        \hline
                        \multicolumn{3}{ p{12cm} }{\textbf{Reduce Type I Error Risk: }Reduce significance level; harder to reject null\newline \textbf{Reduce Type II Error Risk: }Use large sample; ensuring significant spread}\\
			\hline
                        \multicolumn{3}{p{12cm}}{\textbf{Probability} $\mathbf{\alpha}$: Probability of making Type I Error\newline \textbf{Probability} $\mathbf{\beta}$: Probability of making Type II Error}\\
                        \hline
                        \hline
                        \multicolumn{3}{c}{\textbf{Error Examples}}\\
                        \hline
                        \hline
                        \multicolumn{1}{c|}{\textbf{Decision}} & \multicolumn{1}{c|}{H$_0$ is True} & \multicolumn{1}{c}{H$_0$ is False}\\
                        & \multicolumn{1}{|c|}{(Accused is Innocent)} & \multicolumn{1}{c}{(Accused is Guilty)}\\
                        \hline
                        \multicolumn{1}{c|}{Reject H$_0$} & \multicolumn{1}{c|}{\textbf{WRONG} Decision} & \multicolumn{1}{c}{\textbf{CORRECT} Decision}\\
                        \multicolumn{1}{c|}{(Accused Convicted)} & \multicolumn{1}{c|}{(Type I Error)} & \\
                        & \multicolumn{1}{|c|}{Probability $\alpha$} & \\
                        \hline
                        \multicolumn{1}{c|}{Fail-To-Reject H$_0$} & \multicolumn{1}{c|}{\textbf{CORRECT} Decision} & \multicolumn{1}{c}{\textbf{WRONG} Decision}\\
                        \multicolumn{1}{c|}{(Accused Goes Free)} & & \multicolumn{1}{|c}{(Type II Error)}\\
                        & \multicolumn{1}{|c|}{} & \multicolumn{1}{c}{Probability $\beta$}\\
                        \hline
                \end{tabular}
                \end{center}
                \end{table}

		\begin{table}[h]
                        \scriptsize
                        \renewcommand{\arraystretch}{1.25}
                \begin{center}
                \begin{tabular}{p{13cm}}
                        \hline
                        \multicolumn{1}{c}{\textbf{t-stat}}\\
                        \hline
                        \multicolumn{1}{c}{t-stat = $\mathit{\frac{\bar{x}-\bar{\mu}}{\left(\frac{\sigma}{\sqrt{N}}\right)}}$}\\
                        \textbf{$|$t-stat$|$} $>$ 1.96: reject the null hypothesis at the 5\% significance level;\newline \textbf{$|$t-stat$|$} $>$ 2.58: reject the null hypothesis at the 1\% significance level\newline \textit{Given 1000 degrees of freedom}\\
                        \hline
                        \multicolumn{1}{c}{\textbf{p-value}}\\
                        \hline
                        \textbf{p-value} $<$ 0.05: reject the null hypothesis at the 5\% significance level;\newline \textbf{p-value} $<$ 0.01: reject the null hypothesis at the 1\% significance level\newline \textit{Given 1000 degrees of freedom}\\
                        \hline
                        \multicolumn{1}{c}{\textbf{Confidence Relevance}}\\
                        \hline
                        5\% and 1\% significance levels are also referred to as the 95\% and 99\% confidence [in rejecting the null] intervals. Correct syntax: refer to 5\% and 1\% significance levels when referring to p-values; 95\% and 99\% confidence levels when referring to hypotheses.\\
                        \hline
                        \multicolumn{1}{c}{\textbf{Error Relevance}}\\
                        \hline
                        5\% and 1\% levels are used to ensure accuracy and reduced probability of making a Type I error. That is, ``accepting a max 5\%/1\% chance that you are wrong when rejecting the null''; ``you are min 95\%/99\% confident you are right when rejecting''.\\
                        \hline
                \end{tabular}
                \end{center}
                \end{table}

	\newpage

	\subsection{Banks in 2008}

	Banks experienced a lot of cases of Type I Errors prior to the crisis in 2008 and they experienced many cases of making Type II Errors post-2008 crisis. These tended to be centred around the same hypothetical null and alternative hypotheses:\\

	\textbf{H}$\mathbf{_0}$: Mortgage Seeker is Risky\\
	\textbf{H}$\mathbf{_A}$: Mortgage Seeker is Not Risky

		\subsubsection{Pre-Crisis}

	Prior to the 2008 crisis, it could be argued that the hypothetical null hypothesis (H$_0$) is as follows: ``people seeking mortgage loans are risky''; and the corresponding alternative hypothesis (H$\mathrm{_A}$) is: ``people seeking mortgage loans are not risky''. In other words, banks wanted to reject the null hypothesis in favour of the alternative hypothesis so they were able to give out more loans to non-worthy people. That is, to issue sub-prime mortgages, for their own self-interest. However, in practice, banks found this null hypothesis to in fact be true, therefore contradicting their desires. Furthermore, they rejected the null hypothesis even though it was true, making this a Type I Error [Reject H$_0$ when H$_0$ is true]. Therefore, loan application granted where it shouldn't be. Cost of Type I Error = loss of mortgage value plus remaining interest to be paid [assigned: \$500,000 + $x$] upon default.

		\subsubsection{Post-Crisis}

	Post-crisis, the null hypothesis and alternative hypothesis remained the same: ``people seeking mortgage loans are (H$_0$)/are not (H$\mathrm{_A}$) risky''. Banks would simply react in a different manner, with added precaution after observing the domino effect they had helped cause over the course of 2007 and 2008. This time round, banks may refuse to give a perfectly eligible citizen a mortgage loan. That is, they find H$_0$ ``people seeking mortgage loans are risky'' to be false but fail to reject it. Meaning that they are acting as if it is true. In this case they should be favouring the alternative hypothesis but neglect this. This is therefore a Type II Error [Fail-to-reject H$_0$ when H$_0$ is false]. Therefore, loan application aren't granted where they should be. Cost of Type II Error = full potential interest [assigned: $x$ = \$100,000]. By doing this they turn away the opportunity to earn the interest however, it is safer to lose just potential interest (acting in a Type II Error – this case) than it is to lose the full loan in the case of default, plus the remaining potential interest (acting in the Type I Error – pre-2008). But doing this over a long period will have significantly negative effects.\\

	From a more, hypothetical, statistical point of view (and using the post-2008 reaction of banks): say the null hypothesis was as described, the t-test was run and the associated p-value of the t-test was 0.0003 (p-value $<$ 0.01). This would state that ``we should be willing to accept a 0.03\% probability that we are wrong when rejecting H$_0$''. Therefore we would reject the null hypothesis that mortgage seekers are risky. But despite this statistical evidence to the contrary, it was failed to be rejected anyway meaning banks could act in their own (safe) favour. Thus, rejecting mortgage loans when they should grant them; turning away safe customer when they should accept them.

\newpage

\section{Matrix OLS}

	\subsection{Preliminaries}

	Recall basic OLS regression:

	\begin{itemize}
	\setlength\itemsep{0cm}
		\item $\hat{Y}_i=\hat{\alpha}+\hat{\beta}X_i+\varepsilon_i$
		\item $Y=\alpha+\beta_1X_1+\beta_2X_2+...+\beta_KX_K+\varepsilon_i$
		\item $Y=\alpha+\sum_{k=1}^K\beta_kX_k+\varepsilon_i$
		\item $Y$: the dependent variable (what effects $Y$?)
		\item $X_k$: the explanatory variable $k$ (does it effect $Y$?)
		\item $\alpha$: the \textit{y-intercept} (level of $Y$ when $X=0$)
		\item $\beta$: the slope of the line (degree of relationship)
		\item $\varepsilon$: the residuals (random error term)
		\begin{itemize}
			\item Captures explanation not contained within explanatory variables(s)		
		\end{itemize}
		\item For observation $i$
		\item (Hat denotes `estimation')
	\end{itemize}

	Know that:

	\begin{enumerate}
        \setlength\itemsep{0cm}
		\item $\beta=\frac{\Delta Y}{\Delta X}=\frac{\partial Y}{\partial X}$
		\begin{enumerate}
			\item[$-$] ``For each ($+$) unit on the \textit{x} axis, expect the \textit{y} value to $\Delta$ by $\beta$''
		\end{enumerate}
		\item $\varepsilon_i=(Y_i-\hat{Y_i})$
		\begin{enumerate} 
			\item[$-$] Each observation has a predicted $\hat{Y}_i$ on the line of best fit, directly above or below its real $Y_i$
			\item[$-$] Distance between each of these value is the \textit{error}
		\end{enumerate}
		\item This is a \textit{Linear Regression} which uses a straight `line of best fit' (hyperplane)
		\begin{enumerate}
			\item[$-$] For non-linear data, a \textit{Polynomial Model} can be used to account for concave/convex data 
			\item[$-$] Hence: $Y_i=\alpha+\beta_1X_1+\beta_2X_2^2+...+\beta_KX_K^K+\varepsilon_i$
		\end{enumerate}
	\end{enumerate}

	\newpage

	\subsection{Assumptions}	

	\begin{enumerate}
	\setlength\itemsep{0cm}
		\item $\sum\varepsilon_i=0$
		\begin{enumerate}
			\item[$-$] ``The sum of all residuals in the model are equal to zero''
		\end{enumerate}
		\item $var_{\varepsilon_i}=E(\varepsilon_i^2)=\sigma^2$; or $E(\varepsilon\varepsilon')=\sigma^2I$ in vector format
		\begin{enumerate}
			\item[$-$] ``Observations have constant errors''
			\item[$-$] \textit{Homoskedasticity}: constant errors
			\item[$-$] \textit{Heteroskedasticity}: non-constant errors (adjust to \textit{Robust Standard Errors})
			\begin{enumerate}
				\item[$-$] $var_{\varepsilon}=\sigma^2\omega_i^2$
				\item[$-$] For $i\in\{1,...,N\}$
				\item[$-$] The \textit{White Test} is used for heteroskedasticicty testing (H$_0$: Homoskedasticity is present $\therefore$ you wish to fail-to-reject H$_0$; associated p-value $>$ critical value)
			\end{enumerate}
		\end{enumerate}
		\item $cov_{\varepsilon_i,\varepsilon_j}=0\ (\mathrm{for}\ i\neq j)\ \mathrm{or}\ E(X\varepsilon')=0$ in vector format
		\begin{enumerate}
			\item[$-$] ``Error terms should be uncorrelated''
			\item[$-$] Expected observations should be uncorrelated with errors
			\item[$-$] \textit{Instrumental Variable Approach} (IV): where explanatories are correlated with error terms (endogeniety problem), don't use OLS
			\begin{enumerate}
				\item[$-$] $X$ may be \textit{exogenous}, where factors within the model cause changes in $X$; $\therefore\Delta X$ associated w/ $\Delta\varepsilon$
				\item[$-$] \textit{Endogenous} context: variables correlated with error term 
				\item[$-$] \textit{Exogenous} context: variables uncorrelated with error term
				\item[$-$] If two explanatories have high collinearity, omit one
			\end{enumerate}
		\end{enumerate}
		\item (Extra for matrix OLS) $(X'X)^{-1}$ exists
		\begin{enumerate}
			\item[$-$] \textit{Multicollinearity} is not present
				Under multicollinearity, two variables may have high correlation (e.g. $1,-1$)
			\item[$-$] Model would struggle to undertstand which one explains $Y$
			\item[$-$] Hence, w/o multicollinearity, each explanatory gives unique information
		\end{enumerate}
	\end{enumerate}

	\newpage

	\subsection{Beta Estimations}

	$$\hat{Y}_i=\hat{\alpha}+\hat{beta}X_i+\varepsilon_i$$

	$$\therefore\begin{bmatrix}Y_1\\Y2\\\vdots\\Y_N\end{bmatrix}=\begin{bmatrix}\beta_1\\\beta_2\\\vdots\\\beta_N\end{bmatrix}\left[\begin{matrix}1\\1\\\vdots\\1\end{matrix}\begin{matrix}X_{1,1}\\X_{1,2}\\\vdots\\X_{1,N}\end{matrix}\begin{matrix}X_{2,1}\\X_{2,2}\\\vdots\\X_{2,N}\end{matrix}\begin{matrix}\hdots\\\hdots\\\hdots\\\hdots\end{matrix}\begin{matrix}X_{k,1}\\X_{k,2}\\\vdots\\X_{k,N}\end{matrix}\right]+\begin{bmatrix}\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_N\end{bmatrix}$$

	\begin{itemize}
	\setlength\itemsep{0cm}
		\item Going forward, the following are unkown: $\beta,\ \sigma^2$
		\item We wish to maximize the total variation of $\beta_iX_i$ in order to explain $Y$
		\item In result, this will minimise variation of the random error value
		\item In other words, choose $\alpha$ and $\beta$ to $\min\{\sum(Y_i-\hat{Y}_i)^2\}$
		\item Optimization is required [$E(\varepsilon\varepsilon')$ with respect to $\beta$], to the first order
		\item Again, choose $\alpha$ and $\beta$ to:
		\begin{itemize}
			\item $\min\{\sum(Y_i-\hat{Y}_i)^2\}=\min\{E(\varepsilon_i^2)\}=\min\{E(\varepsilon\varepsilon')\}$
			\item Thus, minimizing for \~{}0 @ stationary point
			\item The following uses relevant matrix algebraic notation
			\item Hence, the use of ($Y-\beta X$) to represent the (error) distance of observations from the line of best fit
			\item As $\varepsilon=(Y-\beta X)$ in a vector context:
			\item $$\frac{d(min\{E(\varepsilon\varepsilon')\})}{d\beta}=\frac{d((y-\beta X)(Y-\beta X)')}{d\beta}$$
			\item $$\therefore\frac{d(YY'-Y\beta'X'-X\beta Y'+X\beta\beta'X')}{d\beta}=0$$
			\item $$\therefore\hat{\beta}=(X'X)^{-1}X'Y$$
		\end{itemize}
	\end{itemize}

		\subsubsection{Gauss-Markov Theorem: Beta Estimations}

	This states that, given the aforementioned assumptions, there are no alternative linear estimators of $\beta$'s with a smaller variance. That is, the OLS model is the Best Linear Unbiased Efficient (BLUE) estimator.

	Recall: $\hat{\beta}=(X'X)^{-1}X'Y$, therefore:

	$$\hat{\beta}=(X'X)^{-1}X'(X\beta+\varepsilon)$$
	$$\therefore\hat{\beta}=\beta+(X'X)^{-1}X'\varepsilon$$

	\begin{itemize}
	\setlength\itemsep{0cm}
		\item As $(X'X)^{-1}X'X=I$;
		\item OLS is unbiased given that either [1] $X$ is not stochastic or [2] $X$ is stochastic but independent of $\varepsilon$
		\item Therefore:
		\item $E(\hat{\beta})=E(\beta)+(X'X)^{-1}X'\varepsilon=\beta+(X'X)^{-1}X'E(\varepsilon)$
		\item[] or
		\item $E(\hat{\beta})=E(\beta)+(X'X)^{-1}X'\varepsilon=\beta+(X'X)^{-1}E(X'\varepsilon)$
		\item Therefore, $\hat{\beta}$ is a linear estimator of $\beta$
		\item Where $\beta$ has minimal variance such that: $var_{\beta}-var_{\hat{\beta}}>0$
		\item Meaning that $\hat{\beta}$ estimations should never exceed $\beta$
	\end{itemize}

	\newpage

	\subsection{Variance Estimations}

	\begin{itemize}
	\setlength\itemsep{0cm}
		\item Recall:
		\begin{itemize}
			\item $\hat{\beta}=(X'X)^{-1}X'Y$
			\item $\varepsilon=(Y-\beta Y)$
		\end{itemize}
		\item Estimate $\sigma^2$ with $\hat{\sigma}^2$ in a similar manner to $\beta$
		\item For $N$ observations and $k$ explanatory variables
		\item $\hat{\sigma}^2=\frac{\varepsilon'\varepsilon}{N-k}$
		\item $\therefore\hat{\sigma}^2=\frac{(Y-\beta X)'(Y-\beta X)}{N-K}$
		\item Thus, like $\beta$, $\hat{\sigma}^2$ is an unbiased estimator of $\sigma^2$
	\end{itemize}

		\subsubsection{Variance-Covariance Matrix for $\mathbf{\hat{\beta}}$}
	
	$$E\left((\hat{\beta}-\beta)(\hat{\beta}-\beta)'\right)$$
	$$=E\left(((X'X)^{-1}X'\varepsilon)((X'X)^{-1}X'\varepsilon)'\right)$$
	$$=E\left((X'X)^{-1}X'\varepsilon\varepsilon'X(X'X)^{-1}\right)$$

	Assume $X$ is stochastic and rearrange:

	$$=(X'X)^{-1}X'E(\varepsilon\varepsilon')X(X'X)^{-1}$$

	Recall $E(\varepsilon\varepsilon')=\sigma^2I$:

	$$=(X'X)^{-1}X'(\sigma^2I)X(X'X)^{-1}$$
	$$=\sigma^2I(X'X)^{-1}X'X(X'X)^{-1}$$
	$$=\sigma^2I(X'X)^{-1}\ as\ req.$$
	
	\newpage

	\subsection{Unbiasedness of OLS}

	\begin{itemize}
	\setlength\itemsep{0cm}
		\item Meaning applying $\hat{\beta}$ to all observations will provide a central \textit{true value} $\beta$ on the distribution of $\beta$ values
		\begin{itemize}
			\item Hence $E(\hat{\beta})=\bar{\beta}_i$
		\end{itemize}
		\item $\hat{\beta}=(X'X)^{-1}X$ is referred to as a \textit{projection matrix} which is a vector that captures the error, denoted by $P_x$
	\end{itemize}

	Matrix algebra shown below:

	$$E(\hat{\beta})=\bar{\beta}_i$$
	$$\therefore E(\hat{\beta})=E(\left(X'X)^{-1}X'Y\right)$$
	$$=E(\left(X'X)^{-1}X'(X\beta+\varepsilon)\right)$$
	$$=E(\left(X'X)^{-1}X'X\beta+(X'X)^{-1}X'\varepsilon\right)$$
	$$=E(\left(X'X)^{-1}X'\varepsilon\right)+\beta$$
	$$=\beta\ as\ req.$$

\end{document}
